{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lamtharnhantrakul/anaconda/envs/klustr/lib/python2.7/site-packages/magenta/models/nsynth/wavenet/masked.py:116: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import umap\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from magenta.models.nsynth import utils\n",
    "from magenta.models.nsynth.wavenet import fastgen\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = 'drumData'\n",
    "n_fft = 1024\n",
    "hop_length = n_fft/4\n",
    "use_logamp = False # boost the brightness of quiet sounds\n",
    "reduce_rows = 10 # how many frequency bands to average into one\n",
    "reduce_cols = 1 # how many time steps to average into one\n",
    "crop_rows = 32 # limit how many frequency bands to use\n",
    "crop_cols = 32 # limit how many time steps to use\n",
    "limit = None # set this to 100 to only process 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 ms, sys: 183 ms, total: 184 ms\n",
      "Wall time: 494 ms\n",
      "CPU times: user 655 µs, sys: 10.2 ms, total: 10.9 ms\n",
      "Wall time: 43.8 ms\n",
      "CPU times: user 995 µs, sys: 61.5 ms, total: 62.5 ms\n",
      "Wall time: 251 ms\n",
      "CPU times: user 565 µs, sys: 30.9 ms, total: 31.4 ms\n",
      "Wall time: 132 ms\n",
      "CPU times: user 515 µs, sys: 4.31 ms, total: 4.82 ms\n",
      "Wall time: 17.6 ms\n",
      "CPU times: user 546 µs, sys: 6.09 ms, total: 6.63 ms\n",
      "Wall time: 24.4 ms\n",
      "CPU times: user 1.43 ms, sys: 26.8 ms, total: 28.2 ms\n",
      "Wall time: 69.8 ms\n",
      "(5158, 12000)\n",
      "(422, 12000)\n",
      "(2546, 12000)\n",
      "(1324, 12000)\n",
      "(159, 12000)\n",
      "(228, 12000)\n",
      "(723, 12000)\n"
     ]
    }
   ],
   "source": [
    "drumNames = [\"kick\", \"tom\", \"snare\", \"clap\", \"hi.hat\", \"ride\", \"crash\"]\n",
    "drumFingerPrints = {}\n",
    "drumSamples = {}\n",
    "for d in drumNames:\n",
    "    %time drumSamples[d] = np.load(join(data_root, d+'_samples.npy'))\n",
    "for d in drumNames:\n",
    "    print (drumSamples[d].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wavenet_encode(wave_files):\n",
    "    '''\n",
    "    neural_sample_rate = 16000\n",
    "    audio = utils.load_audio(file_path, \n",
    "                             sample_length=400000, \n",
    "                             sr=neural_sample_rate)\n",
    "    '''\n",
    "    encoding = fastgen.encode(wave_files, './wavenet-ckpt/model.ckpt-200000', 12000)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n",
      "(5158, 12000)\n"
     ]
    }
   ],
   "source": [
    "crashes = drumSamples[\"crash\"]\n",
    "kicks = drumSamples[\"kick\"]\n",
    "\n",
    "sample_kick = kicks[0]\n",
    "sample_crash = crashes[0]\n",
    "print(sample_kick.shape)\n",
    "print(kicks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./wavenet-ckpt/model.ckpt-200000\n"
     ]
    }
   ],
   "source": [
    "wavenet_kick = wavenet_encode(sample_kick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 23, 16)\n",
      "15.8841\n",
      "-8.46548\n"
     ]
    }
   ],
   "source": [
    "print(wavenet_kick.shape)\n",
    "print(np.max(wavenet_kick))\n",
    "print(np.min(wavenet_kick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./wavenet-ckpt/model.ckpt-200000\n"
     ]
    }
   ],
   "source": [
    "small_drumset = [\"tom\", \"hi.hat\", \"ride\", \"crash\"]\n",
    "large_drumset = [\"kick\", \"snare\", \"clap\"]\n",
    "\n",
    "for drum_name in small_drumset: \n",
    "    wavenet_features = wavenet_encode(drumSamples[drum_name])\n",
    "    print (drumName, wavenet_features.shape)  \n",
    "        file_path = './drumEmbeddings/' + drumName + '_wavenet.npy'\n",
    "        np.save(file_path, wavenet_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor drumName in drumNames:\\n    try:\\n        samples = drumSamples[drumName] \\n        samples_wavenet = []\\n        (num_samples, sample_length) = samples.shape # e.g samples.shape=(672,12000)\\n        \\n        for i in tqdm(range(num_samples)):\\n            sample = samples[i]\\n            sample_wavenet = wavenet_encode(sample)\\n            samples_wavenet.append(sample_wavenet)\\n        \\n        #samples_wavenet = wavenet_encode(samples)  # `samples` is a (159,12000) matrix, `samples_wavenet` is (159,23,16)\\n        \\n        samples_wavenet = np.asarray(samples_wavenet)\\n        print (drumName, samples_wavenet.shape)  \\n        file_path = \\'./drumData/\\' + drumName + \\'_wavenet.npy\\'\\n        np.save(file_path, samples_wavenet)\\n    except:\\n            print(\"error!\")\\n            errors += 1\\n        \\nprint(\\'errors:\\', errors)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for drumName in large_drumset:\n",
    "    try:\n",
    "        samples = drumSamples[drumName] \n",
    "        samples_wavenet = []\n",
    "        (num_samples, sample_length) = samples.shape # e.g samples.shape=(672,12000)\n",
    "        \n",
    "        for i in tqdm(range(num_samples)):\n",
    "            sample = samples[i]\n",
    "            sample_wavenet = wavenet_encode(sample)\n",
    "            samples_wavenet.append(sample_wavenet)\n",
    "        \n",
    "        #samples_wavenet = wavenet_encode(samples)  # `samples` is a (159,12000) matrix, `samples_wavenet` is (159,23,16)\n",
    "        \n",
    "        samples_wavenet = np.asarray(samples_wavenet)\n",
    "        print (drumName, samples_wavenet.shape)  \n",
    "        file_path = './drumEmbeddings/' + drumName + '_wavenet.npy'\n",
    "        np.save(file_path, samples_wavenet)\n",
    "    except:\n",
    "            print(\"error!\")\n",
    "            errors += 1\n",
    "        \n",
    "print('errors:', errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hanoi_encode(drumNames, checkpoint_path, sample_length=64000):\n",
    "    # Load up the model for encoding and find the encoding of \"wav_data\"\n",
    "    session_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    batch_size = 1\n",
    "    with tf.Graph().as_default(), tf.Session(config=session_config) as sess:\n",
    "        \n",
    "        hop_length = Config().ae_hop_length\n",
    "        net = hanoi_load_nsynth(batch_size=batch_size, sample_length=sample_length)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        \n",
    "        for drumName in drumNames:\n",
    "            samples = drumSamples[drumName] \n",
    "            samples_wavenet = []\n",
    "            (num_samples, sample_length) = samples.shape # e.g samples.shape=(672,12000)\n",
    "            \n",
    "            for i in tqdm(range(num_samples)):\n",
    "                wav_data = samples[i]\n",
    "                wav_data, sample_length = utils.trim_for_encoding(wav_data, sample_length,hop_length)\n",
    "                encodings = sess.run(net[\"encoding\"], feed_dict={net[\"X\"]: wav_data})\n",
    "                #sample_wavenet = wavenet_encode(sample)\n",
    "                samples_wavenet.append(encodings)\n",
    "        samples_wavenet = np.asarray(samples_wavenet)\n",
    "        print (drumName, samples_wavenet.shape)  \n",
    "        file_path = './drumEmbeddings/' + drumName + '_wavenet.npy'\n",
    "        np.save(file_path, samples_wavenet)\n",
    "\n",
    "def hanoi_load_nsynth(batch_size=1, sample_length=64000):\n",
    "    \"\"\"Load the NSynth autoencoder network.\n",
    "    Args:\n",
    "    batch_size: Batch size number of observations to process. [1]\n",
    "    sample_length: Number of samples in the input audio. [64000]\n",
    "    Returns:\n",
    "    graph: The network as a dict with input placeholder in {\"X\"}\n",
    "    \"\"\"\n",
    "    config = Config()\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        x = tf.placeholder(tf.float32, shape=[batch_size, sample_length])\n",
    "        graph = config.build({\"wav\": x}, is_training=False)\n",
    "        graph.update({\"X\": x})\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from magenta.models.nsynth import utils\n",
    "from magenta.models.nsynth.wavenet.h512_bo16 import Config\n",
    "from magenta.models.nsynth.wavenet.h512_bo16 import FastGenerationConfig\n",
    "\n",
    "def testencode(wav_data, checkpoint_path, sample_length=64000):\n",
    "    if wav_data.ndim == 1:\n",
    "        wav_data = np.expand_dims(wav_data, 0)\n",
    "        batch_size = 1\n",
    "    elif wav_data.ndim == 2:\n",
    "        batch_size = wav_data.shape[0]\n",
    "\n",
    "    # Load up the model for encoding and find the encoding of \"wav_data\"\n",
    "    session_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    with tf.Graph().as_default(), tf.Session(config=session_config) as sess:\n",
    "        hop_length = Config().ae_hop_length\n",
    "        wav_data, sample_length = utils.trim_for_encoding(wav_data, sample_length,\n",
    "                                                      hop_length)\n",
    "        print(sample_length)\n",
    "        net = testload_nsynth(batch_size=batch_size, sample_length=sample_length)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        encodings = sess.run(net[\"encoding\"], feed_dict={net[\"X\"]: wav_data})\n",
    "    return encodings\n",
    "\n",
    "def testload_nsynth(batch_size=1, sample_length=64000):\n",
    "    \"\"\"Load the NSynth autoencoder network.\n",
    "    Args:\n",
    "    batch_size: Batch size number of observations to process. [1]\n",
    "    sample_length: Number of samples in the input audio. [64000]\n",
    "    Returns:\n",
    "    graph: The network as a dict with input placeholder in {\"X\"}\n",
    "    \"\"\"\n",
    "    config = Config()\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        x = tf.placeholder(tf.float32, shape=[batch_size, sample_length])\n",
    "        graph = config.build({\"wav\": x}, is_training=False)\n",
    "        graph.update({\"X\": x})\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11776\n",
      "INFO:tensorflow:Restoring parameters from ./wavenet-ckpt/model.ckpt-200000\n",
      "(5, 23, 16)\n"
     ]
    }
   ],
   "source": [
    "encoding = testencode(kicks[0:5], './wavenet-ckpt/model.ckpt-200000', 12000)\n",
    "print(encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8f53015ba454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhanoi_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_drumset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./wavenet-ckpt/model.ckpt-200000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-2e7915b14627>\u001b[0m in \u001b[0;36mhanoi_encode\u001b[0;34m(drumNames, checkpoint_path, sample_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mhop_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_hop_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_nsynth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-2e7915b14627>\u001b[0m in \u001b[0;36mload_nsynth\u001b[0;34m(batch_size, sample_length)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lamtharnhantrakul/anaconda/envs/klustr/lib/python2.7/site-packages/magenta/models/nsynth/wavenet/h512_bo16.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    231\u001b[0m           \u001b[0mfilter_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mae_filter_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m           name='ae_dilatedconv_%d' % (num_layer + 1))\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       en += masked.conv1d(\n",
      "\u001b[0;32m/Users/lamtharnhantrakul/anaconda/envs/klustr/lib/python2.7/site-packages/magenta/models/nsynth/wavenet/masked.pyc\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(x, num_filters, filter_length, name, dilation, causal, kernel_initializer, biases_initializer)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \"\"\"\n\u001b[1;32m    133\u001b[0m   \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0mkernel_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hanoi_encode(large_drumset, './wavenet-ckpt/model.ckpt-200000', 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
